{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Multi-Layer Perceptron: 2009 American Community Survey\n",
    "\n",
    "### Nick Chao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation (15 points total)\n",
    "\n",
    "[5 points] (mostly the same processes as from previous lab) Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the task is and what parties would be interested in the results. How well would your prediction algorithm need to perform to be considered useful by interested parties?\n",
    "[10 points] (mostly the same processes as from lab one) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification (include a description of any newly formed variables you created).\n",
    "\n",
    "\n",
    "### Evaluation (30 points total)\n",
    "\n",
    "[15 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s generalization performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why does the metric evaluate performance in terms of the business case you argued for. Please note: rarely is accuracy the best evaluation metric to use. \n",
    "Think deeply about an appropriate measure of performance.\n",
    "\n",
    "[15 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time. Convince the reader that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. \n",
    "\n",
    "Important: You should use your chosen evaluation criteria and chosen method for dividing train/test data throughout the report. For example, arguing that f-score is the best evaluation method, but then using accuracy in a grid search will be regarded as a conceptual error and graded accordingly. \n",
    "\n",
    "\n",
    "### Modeling (45 points total)\n",
    "\n",
    "[20 points] Create a custom implementation of the multi-layer perceptron. Start with the implementation given to you in the course. Update the MLP class to:\n",
    "    When instantiated, use a selectable phi function for the initial layer: either sigmoid or linear \n",
    "    Use a selectable cost function when instantiated: either quadratic or cross entropy\n",
    "    Add support for any number of hidden layers (user customizable).\n",
    "\n",
    "[15 points] Tune the hyper-parameters of your MLP model (phi function, objective function, and number of layers). While tuning hyper-parameters, analyze the results using your chosen metric(s) of evaluation. Visualize the evaluation metric(s) versus the hyper-parameters. Conclude what combination of parameters are best.\n",
    "\n",
    "[10 points] Visualize the magnitude of the gradients in each layer of the neural network versus the training iteration. Do the gradients stay consistent in each layer?\n",
    "\n",
    "### Exceptional Work (10 points total)\n",
    "\n",
    "You have free reign to provide additional analyses.\n",
    "One idea (required for 7000 level students):  Implement two more phi functions: ReLU and SiLU (also called Swish). Compare their performance to the linear and sigmoid phi functions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this lab, we investigate possible relationships between an individual's income and attributes about them. The data used in this lab is provided by the 2009 American Community Survey 1-Year PUMS Population File which can be found here. https://catalog.data.gov/dataset/2009-american-community-survey-1-year-pums-population-file. This dataset contains more than 3 million entries and nearly 300 attributes. To make sense of some of these attributes, there is a dictionary lookup that provides more information about the columns. You can find this reference here. https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict09.pdf.\n",
    "\n",
    "To be more specific, we want to see if we can predict a person's current income based on factors about themselves that they might give away when applying for a new job. For example, their age, sex, education level, and more are just a few peices of information that companies may ask for when applying for a new job. \n",
    "\n",
    "This information could be incrediably useful for a company that is hiring new personal. If they know the current income of someone who has applied for a job at their company, then they can offer the lowest starting salary they believe the new canadate will accept. (i.e. slightly above what they currently make). \n",
    "\n",
    "Since my overall goal is considered difficult as it is to accurately determine a person's income, I've decided that I will try to determine a person's income bracket instead. This is a good starting point as there are much less classes to predict. Initially I will start with 5 income brackets. Although this information is less useful than my original goal, it still could help companies with low-balling initial offers to employees. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependancies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.9 s\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%time dataA = pd.read_csv('../data/ss09pusa.csv')\n",
    "%time dataB = pd.read_csv('../data/ss09pusb.csv')\n",
    "merged = pd.concat([dataA,dataB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3030728 entries, 0 to 1466654\n",
      "Columns: 279 entries, RT to pwgtp80\n",
      "dtypes: float64(86), int64(190), object(3)\n",
      "memory usage: 6.3+ GB\n"
     ]
    }
   ],
   "source": [
    "merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2009 American Community Survey has a lot of data, look at all these columns! \n",
      "\n",
      "['RT', 'SERIALNO', 'SPORDER', 'PUMA', 'ST', 'ADJINC', 'PWGTP', 'AGEP', 'CIT', 'CITWP', 'COW', 'DDRS', 'DEAR', 'DEYE', 'DOUT', 'DPHY', 'DRAT', 'DRATX', 'DREM', 'ENG', 'FER', 'GCL', 'GCM', 'GCR', 'HINS1', 'HINS2', 'HINS3', 'HINS4', 'HINS5', 'HINS6', 'HINS7', 'INTP', 'JWMNP', 'JWRIP', 'JWTR', 'LANX', 'MAR', 'MARHD', 'MARHM', 'MARHT', 'MARHW', 'MARHYP', 'MIG', 'MIL', 'MLPA', 'MLPB', 'MLPC', 'MLPD', 'MLPE', 'MLPF', 'MLPG', 'MLPH', 'MLPI', 'MLPJ', 'MLPK', 'NWAB', 'NWAV', 'NWLA', 'NWLK', 'NWRE', 'OIP', 'PAP', 'REL', 'RETP', 'SCH', 'SCHG', 'SCHL', 'SEMP', 'SEX', 'SSIP', 'SSP', 'WAGP', 'WKHP', 'WKL', 'WKW', 'WRK', 'YOEP', 'ANC', 'ANC1P', 'ANC2P', 'DECADE', 'DIS', 'DRIVESP', 'ESP', 'ESR', 'FOD1P', 'FOD2P', 'HICOV', 'HISP', 'INDP', 'JWAP', 'JWDP', 'LANP', 'MIGPUMA', 'MIGSP', 'MSP', 'NAICSP', 'NATIVITY', 'NOP', 'OC', 'OCCP', 'PAOC', 'PERNP', 'PINCP', 'POBP', 'POVPIP', 'POWPUMA', 'POWSP', 'PRIVCOV', 'PUBCOV', 'QTRBIR', 'RAC1P', 'RAC2P', 'RAC3P', 'RACAIAN', 'RACASN', 'RACBLK', 'RACNHPI', 'RACNUM', 'RACSOR', 'RACWHT', 'RC', 'SCIENGP', 'SCIENGRLP', 'SFN', 'SFR', 'SOCP', 'VPS', 'WAOB', 'FAGEP', 'FANCP', 'FCITP', 'FCITWP', 'FCOWP', 'FDDRSP', 'FDEARP', 'FDEYEP', 'FDOUTP', 'FDPHYP', 'FDRATP', 'FDRATXP', 'FDREMP', 'FENGP', 'FESRP', 'FFERP', 'FFODP', 'FGCLP', 'FGCMP', 'FGCRP', 'FHINS1P', 'FHINS2P', 'FHINS3C', 'FHINS3P', 'FHINS4C', 'FHINS4P', 'FHINS5C', 'FHINS5P', 'FHINS6P', 'FHINS7P', 'FHISP', 'FINDP', 'FINTP', 'FJWDP', 'FJWMNP', 'FJWRIP', 'FJWTRP', 'FLANP', 'FLANXP', 'FMARHDP', 'FMARHMP', 'FMARHTP', 'FMARHWP', 'FMARHYP', 'FMARP', 'FMIGP', 'FMIGSP', 'FMILPP', 'FMILSP', 'FOCCP', 'FOIP', 'FPAP', 'FPOBP', 'FPOWSP', 'FRACP', 'FRELP', 'FRETP', 'FSCHGP', 'FSCHLP', 'FSCHP', 'FSEMP', 'FSEXP', 'FSSIP', 'FSSP', 'FWAGP', 'FWKHP', 'FWKLP', 'FWKWP', 'FWRKP', 'FYOEP', 'pwgtp1', 'pwgtp2', 'pwgtp3', 'pwgtp4', 'pwgtp5', 'pwgtp6', 'pwgtp7', 'pwgtp8', 'pwgtp9', 'pwgtp10', 'pwgtp11', 'pwgtp12', 'pwgtp13', 'pwgtp14', 'pwgtp15', 'pwgtp16', 'pwgtp17', 'pwgtp18', 'pwgtp19', 'pwgtp20', 'pwgtp21', 'pwgtp22', 'pwgtp23', 'pwgtp24', 'pwgtp25', 'pwgtp26', 'pwgtp27', 'pwgtp28', 'pwgtp29', 'pwgtp30', 'pwgtp31', 'pwgtp32', 'pwgtp33', 'pwgtp34', 'pwgtp35', 'pwgtp36', 'pwgtp37', 'pwgtp38', 'pwgtp39', 'pwgtp40', 'pwgtp41', 'pwgtp42', 'pwgtp43', 'pwgtp44', 'pwgtp45', 'pwgtp46', 'pwgtp47', 'pwgtp48', 'pwgtp49', 'pwgtp50', 'pwgtp51', 'pwgtp52', 'pwgtp53', 'pwgtp54', 'pwgtp55', 'pwgtp56', 'pwgtp57', 'pwgtp58', 'pwgtp59', 'pwgtp60', 'pwgtp61', 'pwgtp62', 'pwgtp63', 'pwgtp64', 'pwgtp65', 'pwgtp66', 'pwgtp67', 'pwgtp68', 'pwgtp69', 'pwgtp70', 'pwgtp71', 'pwgtp72', 'pwgtp73', 'pwgtp74', 'pwgtp75', 'pwgtp76', 'pwgtp77', 'pwgtp78', 'pwgtp79', 'pwgtp80']\n",
      "\n",
      "No wonder they provide a reference dictionary to figure out what all these acronyms mean\n"
     ]
    }
   ],
   "source": [
    "print('The 2009 American Community Survey has a lot of data, look at all these columns! \\n\\n'+str(list(merged.columns.values)))\n",
    "print('\\nNo wonder they provide a reference dictionary to figure out what all these acronyms mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously there is way more data here than we need so lets start by reducing the number of columns to only what we consider useful for our classification. The following attributes will remain as they are peices of information that new hires might give away when applying for a job.\n",
    "\n",
    "Citizenship, age, class of work, English fluentcy, martial status, military status, sex, education background, disability status, race, geographical location. We will also keep the individual's income as it is the attribute that we are attempting to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3030728 entries, 0 to 1466654\n",
      "Data columns (total 13 columns):\n",
      "CIT      int64\n",
      "AGEP     int64\n",
      "COW      float64\n",
      "ENG      float64\n",
      "MAR      int64\n",
      "MIL      float64\n",
      "SCHL     float64\n",
      "SEX      int64\n",
      "DIS      int64\n",
      "PINCP    float64\n",
      "POWSP    float64\n",
      "RAC1P    int64\n",
      "FOD1P    float64\n",
      "dtypes: float64(7), int64(6)\n",
      "memory usage: 323.7 MB\n"
     ]
    }
   ],
   "source": [
    "cols_to_save = ['CIT','AGEP','COW','ENG','MAR','MIL','SCHL','SEX','DIS','PINCP','POWSP','RAC1P','FOD1P']\n",
    "new_data = merged.filter(items=cols_to_save)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIT</th>\n",
       "      <th>AGEP</th>\n",
       "      <th>COW</th>\n",
       "      <th>ENG</th>\n",
       "      <th>MAR</th>\n",
       "      <th>MIL</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>DIS</th>\n",
       "      <th>PINCP</th>\n",
       "      <th>POWSP</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>FOD1P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>36800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54600.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>56000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>41000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>14200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>66000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6209.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>97520.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3606.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466625</th>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466626</th>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466627</th>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466628</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466629</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466630</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466631</th>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466632</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466633</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466634</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466635</th>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>28700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466636</th>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466637</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>53400.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466638</th>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466639</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>41100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466640</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>46000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466641</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466642</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13020.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466643</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466644</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36500.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466645</th>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466646</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466647</th>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466648</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>140000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466649</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466650</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466651</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-4800.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466652</th>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>39000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466653</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466654</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3030728 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CIT  AGEP  COW  ENG  MAR  MIL  SCHL  SEX  DIS     PINCP  POWSP  \\\n",
       "0          1    51  NaN  NaN    4  3.0  16.0    1    1    3800.0    NaN   \n",
       "1          1    64  1.0  NaN    1  5.0  19.0    2    1   36800.0    1.0   \n",
       "2          1    68  1.0  NaN    1  3.0  14.0    1    1   54600.0    1.0   \n",
       "3          1    61  1.0  NaN    3  5.0  16.0    2    2    6000.0    1.0   \n",
       "4          1    38  1.0  NaN    5  5.0  16.0    1    2   14000.0    1.0   \n",
       "5          1    65  6.0  NaN    1  5.0  16.0    2    1   13000.0    NaN   \n",
       "6          1    74  1.0  NaN    1  3.0  16.0    1    1   45200.0    1.0   \n",
       "7          1    23  1.0  NaN    5  5.0  12.0    2    1     820.0    NaN   \n",
       "8          1    42  1.0  NaN    1  5.0  19.0    2    2   25200.0    1.0   \n",
       "9          1    42  1.0  NaN    1  5.0  13.0    1    2   56000.0    1.0   \n",
       "10         1    23  1.0  NaN    3  5.0  18.0    2    2       0.0    NaN   \n",
       "11         1     3  NaN  NaN    5  NaN   2.0    2    2       NaN    NaN   \n",
       "12         1    35  3.0  NaN    3  5.0  19.0    1    2   45000.0    1.0   \n",
       "13         1    29  1.0  NaN    3  5.0  19.0    2    2     300.0    NaN   \n",
       "14         1    39  1.0  NaN    1  5.0  16.0    1    2   41000.0    1.0   \n",
       "15         1    38  1.0  NaN    1  5.0  16.0    2    2    3500.0    1.0   \n",
       "16         1    14  NaN  NaN    5  NaN  11.0    1    2       NaN    NaN   \n",
       "17         1    13  NaN  NaN    5  NaN   9.0    2    2       NaN    NaN   \n",
       "18         1    12  NaN  NaN    5  NaN   8.0    2    2       NaN    NaN   \n",
       "19         1    32  NaN  NaN    5  5.0  12.0    1    2    1500.0    NaN   \n",
       "20         1    34  4.0  NaN    5  5.0  16.0    2    2   14200.0    1.0   \n",
       "21         1    14  NaN  NaN    5  NaN  10.0    2    2       NaN    NaN   \n",
       "22         1    12  NaN  NaN    5  NaN   8.0    1    2       NaN    NaN   \n",
       "23         1    72  NaN  NaN    1  3.0  17.0    1    1   12000.0    NaN   \n",
       "24         1    60  1.0  NaN    1  5.0  16.0    2    2       0.0    NaN   \n",
       "25         1    39  7.0  NaN    3  5.0  19.0    1    1   20500.0    1.0   \n",
       "26         1    53  1.0  NaN    1  5.0  21.0    2    2   66000.0    1.0   \n",
       "27         1    54  1.0  NaN    1  5.0  19.0    1    2   45000.0    1.0   \n",
       "28         1    21  1.0  NaN    5  5.0  16.0    1    2    6000.0    NaN   \n",
       "29         1    51  2.0  NaN    5  5.0  24.0    1    2   97520.0    1.0   \n",
       "...      ...   ...  ...  ...  ...  ...   ...  ...  ...       ...    ...   \n",
       "1466625    1    75  NaN  NaN    2  5.0  16.0    2    2   37500.0    NaN   \n",
       "1466626    5    30  1.0  2.0    1  5.0   9.0    1    2   50000.0   56.0   \n",
       "1466627    5    26  NaN  2.0    1  5.0  15.0    2    2       0.0    NaN   \n",
       "1466628    1     4  NaN  NaN    5  NaN   1.0    1    2       NaN    NaN   \n",
       "1466629    1     2  NaN  NaN    5  NaN   NaN    2    2       NaN    NaN   \n",
       "1466630    1    48  1.0  NaN    1  5.0  21.0    1    2   80000.0   56.0   \n",
       "1466631    1    53  1.0  NaN    1  5.0  19.0    2    2    3000.0   56.0   \n",
       "1466632    1    25  1.0  NaN    5  5.0  18.0    1    2    3700.0   30.0   \n",
       "1466633    1    93  NaN  NaN    2  5.0  16.0    2    1   12000.0    NaN   \n",
       "1466634    1    64  4.0  NaN    2  5.0  16.0    2    1   10800.0    NaN   \n",
       "1466635    1    81  NaN  NaN    1  3.0  19.0    1    2   28700.0    NaN   \n",
       "1466636    1    78  NaN  NaN    1  5.0  16.0    2    2    6800.0    NaN   \n",
       "1466637    1    79  NaN  NaN    2  3.0  21.0    1    2   53400.0    NaN   \n",
       "1466638    1    87  NaN  NaN    2  5.0  12.0    2    1    4200.0    NaN   \n",
       "1466639    1    79  NaN  NaN    2  5.0  18.0    2    2   41100.0    NaN   \n",
       "1466640    1    49  2.0  NaN    1  5.0  18.0    2    2   46000.0   56.0   \n",
       "1466641    1    55  1.0  NaN    1  5.0  16.0    1    2   22000.0   56.0   \n",
       "1466642    1    24  3.0  NaN    1  5.0  21.0    2    2   13020.0   56.0   \n",
       "1466643    1     1  NaN  NaN    5  NaN   NaN    2    2       NaN    NaN   \n",
       "1466644    1    29  3.0  NaN    1  5.0  21.0    1    2   36500.0   56.0   \n",
       "1466645    1    56  NaN  NaN    3  5.0  18.0    2    2    6000.0    NaN   \n",
       "1466646    1    42  1.0  NaN    3  5.0  21.0    2    2    4000.0   56.0   \n",
       "1466647    1    41  1.0  NaN    1  5.0  18.0    2    2       0.0    NaN   \n",
       "1466648    1    42  1.0  NaN    1  3.0  19.0    1    2  140000.0   56.0   \n",
       "1466649    1     2  NaN  NaN    5  NaN   NaN    1    2       NaN    NaN   \n",
       "1466650    1    50  NaN  NaN    3  4.0  21.0    1    1   15000.0    NaN   \n",
       "1466651    1    60  6.0  NaN    1  3.0  19.0    1    2   -4800.0   56.0   \n",
       "1466652    1    58  6.0  NaN    1  5.0  19.0    2    2   39000.0   56.0   \n",
       "1466653    1    24  3.0  NaN    1  5.0  19.0    2    2   23000.0   56.0   \n",
       "1466654    1    23  1.0  1.0    1  5.0  19.0    1    2   10000.0   56.0   \n",
       "\n",
       "         RAC1P   FOD1P  \n",
       "0            1     NaN  \n",
       "1            1     NaN  \n",
       "2            1     NaN  \n",
       "3            2     NaN  \n",
       "4            2     NaN  \n",
       "5            1     NaN  \n",
       "6            1     NaN  \n",
       "7            1     NaN  \n",
       "8            1     NaN  \n",
       "9            1     NaN  \n",
       "10           1     NaN  \n",
       "11           1     NaN  \n",
       "12           1     NaN  \n",
       "13           1     NaN  \n",
       "14           1     NaN  \n",
       "15           1     NaN  \n",
       "16           1     NaN  \n",
       "17           1     NaN  \n",
       "18           1     NaN  \n",
       "19           2     NaN  \n",
       "20           2     NaN  \n",
       "21           2     NaN  \n",
       "22           2     NaN  \n",
       "23           1     NaN  \n",
       "24           1     NaN  \n",
       "25           1     NaN  \n",
       "26           2  6209.0  \n",
       "27           2     NaN  \n",
       "28           2     NaN  \n",
       "29           1  3606.0  \n",
       "...        ...     ...  \n",
       "1466625      1     NaN  \n",
       "1466626      1     NaN  \n",
       "1466627      1     NaN  \n",
       "1466628      1     NaN  \n",
       "1466629      1     NaN  \n",
       "1466630      1  3600.0  \n",
       "1466631      1     NaN  \n",
       "1466632      1     NaN  \n",
       "1466633      1     NaN  \n",
       "1466634      1     NaN  \n",
       "1466635      1     NaN  \n",
       "1466636      1     NaN  \n",
       "1466637      1  5501.0  \n",
       "1466638      1     NaN  \n",
       "1466639      1     NaN  \n",
       "1466640      1     NaN  \n",
       "1466641      1     NaN  \n",
       "1466642      1  2304.0  \n",
       "1466643      1     NaN  \n",
       "1466644      1  1103.0  \n",
       "1466645      1     NaN  \n",
       "1466646      1  6107.0  \n",
       "1466647      9     NaN  \n",
       "1466648      1     NaN  \n",
       "1466649      1     NaN  \n",
       "1466650      1  1105.0  \n",
       "1466651      1     NaN  \n",
       "1466652      1     NaN  \n",
       "1466653      1     NaN  \n",
       "1466654      1     NaN  \n",
       "\n",
       "[3030728 rows x 13 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We began with over 3 million records and 279 differnt attributes provided in the survey. Of the nearly 300 attributes that were provided in the dataset, we reduced it down to the thirteen shown above. The table below shows the desired attributes and examples of what we would want the data to look like. \n",
    "\n",
    "|Attribute|Description|Type|Example|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "| CIT | Citizenship Status | Int | 1. Citizen, 0. Non-citizen |\n",
    "| AGEP | Age | Int | 23\n",
    "| COW | Class of Worker | Float | 3. Local Government, 4. State Government |\n",
    "| ENG | Ability to speak English  | Int | 1. Speaks English, 0. Doesn't Speak English |\n",
    "| MAR | Marital Status | Int | 1. Married, 2. Widowed |\n",
    "| MIL | Military Service | Int | 1. Yes, 0. No |\n",
    "| SCHL | Educational Attainment  | Float | 21 Bachelor's Degree, 22 Master's Degree |\n",
    "| SEX | Sex      | Int | True. Male |\n",
    "| DIS | Disability | Int | True. Disabled |\n",
    "| PINCP | Total Person's Income | Float\n",
    "| POWSP | Place of work | Float | 048 Texas, 049 Utah |\n",
    "| RAC1P | Detailed Race Code | Int | 1 White, 6 Asian |\n",
    "| FOD1P | Field of Degree | Float | 2407 Computer Engineering, 2408 Electrical Engineering |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "The next step is data cleaning and modifying the data into more useful data types. Lets start by modifying some of these data types into more useful and proper variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change citizenship to Int.\n",
    "# 1-4 is a citizen (true) and 5 is not a citizen (false)\n",
    "\n",
    "new_data.CIT.replace(to_replace = range(5),\n",
    "                    value=[1,1,1,1,0],\n",
    "                    inplace=True)\n",
    "new_data['CIT'] = new_data['CIT'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Ability to Speak English to boolean\n",
    "# b is N/A but it would be a good assumption to assume they speak English\n",
    "new_data['ENG']=new_data['ENG'].fillna(1)\n",
    "# 1-2 speaks English well or very well, 3-4 speaks English not well or not at all.\n",
    "new_data.ENG.replace(to_replace = range(4),\n",
    "                    value=[1,1,0,0],\n",
    "                    inplace=True)\n",
    "new_data['ENG'] = new_data['ENG'].astype('int')# Change Military Status to Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b is N/A because less than 17 years old so lets just change this to 0\n",
    "new_data['MIL']=new_data['MIL'].fillna(0)\n",
    "# 1-3 Yes, 4-5 No\n",
    "new_data.MIL.replace(to_replace = range(5),\n",
    "                    value=[1,1,1,0,0],\n",
    "                    inplace=True)\n",
    "new_data['MIL'] = new_data['MIL'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change Sex to Int\n",
    "# # 1 is male, 2 is female. Changing 2 to 0 for boolean conversion\n",
    "# new_data.SEX.replace(to_replace = range(2),\n",
    "#                     value=[1,0],\n",
    "#                     inplace=True)\n",
    "# new_data['SEX'] = new_data['SEX'].astype('Int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change DIS to Int\n",
    "# # 1 is disabled, 2 is no disability. Changing 2 to 0 for boolean conversion\n",
    "# new_data.DIS.replace(to_replace = range(2),\n",
    "#                     value=[1,0],\n",
    "#                     inplace=True)\n",
    "# new_data['DIS'] = new_data['DIS'].astype('Int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Educational Atttainment to INT\n",
    "# bb is N/A for less than 3 years old.\n",
    "new_data['SCHL']=new_data['SCHL'].fillna(0)\n",
    "# For this classification lets simplify some of these education levels.\n",
    "# 0 between No schooling and Grade 8\n",
    "# 1 between Grade 9 and Grade 12 no diploma\n",
    "# 2 for High School degree or GED\n",
    "# 3 Some college to Associate's degree\n",
    "# 4 Bachelor's Degree\n",
    "# 5 Master's Degree\n",
    "# 6 Professional degree or Doctorate\n",
    "new_data.SCHL.replace(to_replace = range(25),\n",
    "                    value=[0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,2,2,3,3,3,4,5,6,6],\n",
    "                    inplace=True)\n",
    "new_data['SCHL'] = new_data['SCHL'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets remove all entries with people under the age of 18 because our goal is to focus on personal income of working class individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete younger than 18\n",
    "new_data = new_data[new_data.AGEP >= 18]\n",
    "#new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets work on the Nulls..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that contain nulls: ['COW', 'POWSP', 'FOD1P']\n"
     ]
    }
   ],
   "source": [
    "# find null columns\n",
    "print('Columns that contain nulls: '+str(new_data.columns[new_data.isnull().any()].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Field of Study, we can replace all the Nulls with 0s since they only refer to those with at least a college degree\n",
    "Let's remove any Class of Worker and Place of Work rows with Nulls since those entres are for idividuals who have not worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field of Study  -> 0\n",
    "# Class of Worker -> Remove if Null\n",
    "# Place of Work   -> Remove if Null\n",
    "\n",
    "new_data['FOD1P'].fillna(0, inplace=True)\n",
    "new_data = new_data[pd.notnull(new_data['COW'])]\n",
    "new_data = new_data[pd.notnull(new_data['POWSP'])]\n",
    "\n",
    "# Convert the Floats to Ints\n",
    "# COW, POWSP, FOD1P, PINCP\n",
    "\n",
    "new_data['COW'] = new_data['COW'].astype('int')\n",
    "new_data['POWSP'] = new_data['POWSP'].astype('int')\n",
    "new_data['FOD1P'] = new_data['FOD1P'].astype('int')\n",
    "new_data['PINCP'] = new_data['PINCP'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that contain nulls: []\n"
     ]
    }
   ],
   "source": [
    "# Double check for null columns\n",
    "print('Columns that contain nulls: '+str(new_data.columns[new_data.isnull().any()].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by splitting the personal income data into 2 different classes to test Binary Logistic Regression. Later, we will split the personal income into 5 different classes. This will help verify our income predictions while also being specific enough to work with our business goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]\n",
       "Categories (2, int64): [0 < 1]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_data = new_data.copy(deep=False) # saving a copy for later\n",
    "\n",
    "new_data['PINCP'] = pd.qcut(new_data.PINCP, 2, labels=[0,1])\n",
    "new_data['PINCP'].unique()\n",
    "\n",
    "#qcut(x, q, labels=None, retbins=False, precision=3, duplicates='raise')\n",
    "#cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False)\n",
    "\n",
    "#new_data['PINCP'] = pd.qcut(new_data.PINCP, 5, labels=[0,1,2,3,4])\n",
    "#new_data['PINCP'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['PINCP'] = new_data['PINCP'].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people in each class:\n",
      "1: 660205\n",
      "0: 678074\n"
     ]
    }
   ],
   "source": [
    "# Lets see how the income classes have split...\n",
    "print('Number of people in each class:')\n",
    "for value in new_data.PINCP.unique(): \n",
    "    print(str(value)+': ' +str(len(new_data[new_data['PINCP'] == value])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1338279 entries, 1 to 1466654\n",
      "Data columns (total 13 columns):\n",
      "Citizenship        1338279 non-null int32\n",
      "Age                1338279 non-null int64\n",
      "Class of Work      1338279 non-null int32\n",
      "Speaks English     1338279 non-null int32\n",
      "Martial Status     1338279 non-null int64\n",
      "Military Status    1338279 non-null int32\n",
      "Education Level    1338279 non-null int32\n",
      "Male               1338279 non-null int64\n",
      "Disabled?          1338279 non-null int64\n",
      "Income             1338279 non-null int32\n",
      "Place of Work      1338279 non-null int32\n",
      "Race               1338279 non-null int64\n",
      "Field of Study     1338279 non-null int32\n",
      "dtypes: int32(8), int64(5)\n",
      "memory usage: 102.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's rename some of these columns so they make more sense.\n",
    "new_data.rename(columns={'CIT': 'Citizenship','AGEP': 'Age','COW': 'Class of Work','ENG': 'Speaks English','MAR': 'Martial Status','MIL': 'Military Status','SCHL': 'Education Level','SEX': 'Male','DIS': 'Disabled?','PINCP': 'Income','POWSP': 'Place of Work','RAC1P': 'Race','FOD1P': 'Field of Study'}, inplace=True)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Since my goal is to accurately determine an individual's income bracket (and offer a salary slightly above what they currently make), I need to use an appropriate metric for evaluating the classifier. In this specific senerio, there is only one case where I could deem this a successful classifier, as the model correctly picks the right income bracket.\n",
    "\n",
    "If the model predicts an income bracket below the actual bracket than the company would potentially be offering the applicant an extremely low salary and would certainly lose the canadiate. On the other hand, if the model predicts a bracket greater than the actual bracket than the company would offer way more money than neccessary. \n",
    "\n",
    "#### F-Score\n",
    "I've decided to use F-score as my metric for scoring the predictive model. F-Score uses a combination of both Recall and Precision. \n",
    "\n",
    "##### Recall \n",
    "A good recall is the ability to mark as many of the income brackets as the desired income bracket. For example, if we were looking for the middle income bracket a good recall would mark as many of them in the middle income bracket as possible. Furthermore, a perfect recall model, when given a class, will identify every individual who is in the middle income bracket as a middle income bracket individual. However, it should be noted that this does not mean that every class that is marked as a middle income bracket individual is actually a middle income bracket individual. This metric is specifically focused and conerned with reducing false negatives.\n",
    "\n",
    "In this case, good recall is the ability for model to identify all middle income individuals as middle income individuals or bottom income bracker individuals as bottom income bracket individuals etc.\n",
    "\n",
    "##### Precision\n",
    "\n",
    "Precision compliments recall as it concerns whether the classifier was correct or not. For example, a model with high precision on a certain class will be accurate when it claims that a test case is of that class. Perfect precision means you can trust that the model knows when a \"duck is a duck\". However, precision does not concern itself with ensuring that it identifies all the classes correctly. \n",
    "\n",
    "In this case, good precision is the abillity for the model to be correct when it claims an individual is in this specific bracket. It is not concerned with catching all individuals in a specific bracket, only that when it determines a individual is a specific bracket, it is correct. \n",
    "\n",
    "#### Additional metric\n",
    "\n",
    "The combination of Recall and Precision make F-score an excellent metric for judging models. However, F-score is designed for binary classifiers. Since we have multiple classes we need to figure out how to utilize F-score for all the classes.\n",
    "\n",
    "In order to achieve this, we will use F-score on all of the classes, using multiple training and test sets. that are weighted to be evenly distributed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "\n",
    "With regards to splitting the data for training and testing purposes, I am going to split the data into 80% for training and 20% for testing, ensuring that both sets are equally portional. Since my data set is very large I should not have a problem being able to do this and cross validation should not be neccessary as there is over a million entres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start by making a copy of the cleaned data we are using.\n",
    "new_df = new_data.copy()\n",
    "if 'Income' in new_data:\n",
    "    y = new_df['Income'].values    # Since Income is our target class, lets make a copy of it\n",
    "    del new_df['Income']           # Now we need to remove the target class\n",
    "    X = new_df.values              # The remaining data will be used to train\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "# # Scikit Learns provides a way to split our data into training and testing subsets.\n",
    "# cv_object = ShuffleSplit(train_size=.8, test_size=0.2, n_splits=1)\n",
    "                         \n",
    "#print(cv_object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with the implementation of the two layer perception from\n",
    "# https://github.com/eclarson/MachineLearningNotebooks/blob/master/07.%20MLP%20Neural%20Networks.ipynb\n",
    "\n",
    "# Below is a modified implementation for multiple layers\n",
    "\n",
    "class MultiLayerPerceptron(object):\n",
    "    def __init__(self, n_hidden=30, n_hidden_layers=2,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None, \n",
    "                 cost_function='quadratic', activation_method='sigmoid'):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.cost_function = cost_function\n",
    "        self.activation_method = activation_method\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        Ws = [None]*self.n_hidden_layers\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            #if i!=self.n_hidden_layers-1:\n",
    "            if i==0:\n",
    "                Wi_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "                Wi = np.random.uniform(-1.0, 1.0,size=Wi_num_elems)\n",
    "                Wi = Wi.reshape(self.n_hidden, self.n_features_ + 1)\n",
    "            elif i==self.n_hidden_layers-1:\n",
    "                Wi_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "                Wi = np.random.uniform(-1.0, 1.0,size=Wi_num_elems)\n",
    "                Wi = Wi.reshape(self.n_output_, self.n_hidden + 1)\n",
    "            else:\n",
    "                Wi_num_elems = (self.n_hidden + 1)*self.n_hidden\n",
    "                Wi = np.random.uniform(-1.0, 1.0,size=Wi_num_elems)\n",
    "                Wi = Wi.reshape(self.n_hidden, self.n_hidden + 1)\n",
    "            \n",
    "            Ws[i] = Wi\n",
    "        return Ws\n",
    "    \n",
    "\n",
    "    def _sigmoid(self,z):\n",
    "        return expit(z)\n",
    "    \n",
    "    def _activation(self,z):\n",
    "        if self.activation_method=='sigmoid':\n",
    "            return expit(z)\n",
    "        elif self.activation_method=='linear':\n",
    "            return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, Ws):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        meansquaresum = 0\n",
    "        for Wi in Ws:\n",
    "            meansquaresum += np.mean(Wi[:, 1:]**2)\n",
    "        return (lambda_/2.0) * np.sqrt(meansquaresum)\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,Ws):\n",
    "        if self.cost_function=='quadratic':\n",
    "            return self._quad_cost(A3,Y_enc,Ws)\n",
    "        elif self.cost_function=='cross':\n",
    "            return self._cross_cost(A3,Y_enc,Ws)\n",
    "    \n",
    "    def _quad_cost(self,A3,Y_enc,Ws):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, Ws)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _cross_cost(self,A3,Y_enc,Ws):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, Ws)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, Ws):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        As = [None]*(self.n_hidden_layers+1)\n",
    "        Zs = [None]*self.n_hidden_layers\n",
    "        for i in range(len(As)):\n",
    "            if i==0:\n",
    "                As[0] = self._add_bias_unit(X, how='column')\n",
    "                As[0] = As[0].T\n",
    "            else:\n",
    "                Zs[i-1] = Ws[i-1] @ As[i-1]\n",
    "                if i!=len(As)-1:\n",
    "                    As[i] = self._activation(Zs[i-1])\n",
    "                    As[i] = self._add_bias_unit(As[i], how='row')\n",
    "                else:\n",
    "                    As[i] = self._sigmoid(Zs[i-1])\n",
    "        return As, Zs\n",
    "    \n",
    "    def _get_gradient(self, As, Zs, Y_enc, Ws):\n",
    "        if self.cost_function=='quadratic':\n",
    "            return self._quad_gradient(As,Zs,Y_enc,Ws)\n",
    "        elif self.cost_function=='cross':\n",
    "            return self._cross_gradient(As,Zs,Y_enc,Ws)\n",
    "        \n",
    "    def _quad_gradient(self, As, Zs, Y_enc, Ws):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        Vs = [None]*self.n_hidden_layers\n",
    "        grads = [None]*self.n_hidden_layers\n",
    "        for i in range(self.n_hidden_layers,0,-1):\n",
    "            if i==self.n_hidden_layers:\n",
    "                Vs[i-1] = -2*(Y_enc-As[i])*As[i]*(1-As[i])\n",
    "                grads[i-1] = Vs[i-1] @ As[i-1].T\n",
    "            elif self.n_hidden_layers > 2 and i <= self.n_hidden_layers - 2 and i > 0:\n",
    "                if self.activation_method == 'sigmoid':\n",
    "                    Vs[i-1] = As[i]*(1-As[i])*(Ws[i].T @ Vs[i][1:,:])\n",
    "                elif self.activation_method == 'linear':\n",
    "                    Vs[i-1] = (Ws[i].T @ Vs[i][1:,:])\n",
    "                elif self.activation_method == 'relu':\n",
    "                    Vs[i-1] = Ws[i].T @ Vs[i][1:,:]\n",
    "                    Vs[i-1][Zs[i-1]<=0] = 0    \n",
    "                grads[i-1] = Vs[i-1][1:,:] @ As[i-1].T\n",
    "            else:\n",
    "                if self.activation_method == 'sigmoid':\n",
    "                    Vs[i-1] = As[i]*(1-As[i])*(Ws[i].T @ Vs[i])\n",
    "                elif self.activation_method == 'linear':\n",
    "                    Vs[i-1] = (Ws[i].T @ Vs[i])\n",
    "                grads[i-1] = Vs[i-1][1:,:] @ As[i-1].T\n",
    "        return grads\n",
    "\n",
    "    def _cross_gradient(self, As, Zs, Y_enc, Ws):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        \n",
    "        Vs = [None]*self.n_hidden_layers\n",
    "        grads = [None]*self.n_hidden_layers\n",
    "        for i in range(self.n_hidden_layers,0,-1):\n",
    "            if i==self.n_hidden_layers:\n",
    "                Vs[i-1] = As[i]-Y_enc\n",
    "                grads[i-1] = Vs[i-1] @ As[i-1].T\n",
    "            elif self.n_hidden_layers > 2 and i <= self.n_hidden_layers - 2 and i > 0:\n",
    "                if self.activation_method == 'sigmoid':\n",
    "                    Vs[i-1] = As[i]*(1-As[i])*(Ws[i].T @ Vs[i][1:,:])\n",
    "                elif self.activation_method == 'linear':\n",
    "                    Vs[i-1] = (Ws[i].T @ Vs[i][1:,:])\n",
    "                grads[i-1] = Vs[i-1][1:,:] @ As[i-1].T\n",
    "            else:\n",
    "                if self.activation_method == 'sigmoid':\n",
    "                    Vs[i-1] = As[i]*(1-As[i])*(Ws[i].T @ Vs[i])\n",
    "                elif self.activation_method == 'linear':\n",
    "                    Vs[i-1] = (Ws[i].T @ Vs[i])\n",
    "                grads[i-1] = Vs[i-1][1:,:] @ As[i-1].T\n",
    "        return grads\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        As,_ = self._feedforward(X, self.Ws)\n",
    "        y_pred = np.argmax(As[-1], axis=0)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.Ws = self._initialize_weights()\n",
    "        #gradients holds the gradients at each layer and iteration, but does not correctly separate them for some reason\n",
    "        gradients = [[]]*self.n_hidden_layers\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            As, Zs = self._feedforward(X_data,self.Ws)\n",
    "            \n",
    "            cost = self._cost(As[-1],Y_enc,self.Ws)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grads = self._get_gradient(As=As, Zs=Zs, Y_enc=Y_enc,\n",
    "                                              Ws=self.Ws)\n",
    "            for j in range(len(self.Ws)):\n",
    "                self.Ws[j] -= self.eta * grads[j]\n",
    "                gradients[j].append(np.mean(grads[j]))\n",
    "        self.gradients_=[]\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            self.gradients_.append(gradients[0][i:][::self.n_hidden_layers])\n",
    "                \n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param value is used to create a MultiLayerPerceptron object\n",
    "# Note the easily modifiable layers, cost function and activation method\n",
    "\n",
    "params = dict(n_hidden=10, \n",
    "              n_hidden_layers=3,\n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=500, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1,\n",
    "              cost_function='quadratic',\n",
    "              activation_method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 500/500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.5045244642376782\n",
      "F1 acc:  0.45529321393375305\n",
      "Wall time: 3min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "mlp = MultiLayerPerceptron(**params)\n",
    "mlp.fit(X_train, y_train, print_progress=10)\n",
    "yhat = mlp.predict(X_test)\n",
    "print('Test acc:',accuracy_score(y_test,yhat))\n",
    "sample_weights = [1.5,1.0,2.0,2.5]\n",
    "weights = [sample_weights[x] for x in y_test ]\n",
    "print('F1 acc: ', f1_score(y_test,yhat,average='weighted', sample_weight=weights) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-paraameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: quadratic | Activation: sigmoid | Layers: 2 | F1 Score: 0.22433801546936477 | Accuracy: 0.4954755357623218\n",
      "Cost: quadratic | Activation: sigmoid | Layers: 3 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: quadratic | Activation: sigmoid | Layers: 4 | F1 Score: 0.22433801546936477 | Accuracy: 0.4954755357623218\n",
      "Cost: quadratic | Activation: sigmoid | Layers: 5 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: quadratic | Activation: sigmoid | Layers: 6 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: quadratic | Activation: sigmoid | Layers: 7 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: quadratic | Activation: linear | Layers: 2 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: quadratic | Activation: linear | Layers: 3 | F1 Score: 0.6865919602019581 | Accuracy: 0.6631609229757599\n",
      "Cost: quadratic | Activation: linear | Layers: 4 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: quadratic | Activation: linear | Layers: 5 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: quadratic | Activation: linear | Layers: 6 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: quadratic | Activation: linear | Layers: 7 | F1 Score: 0.4616180653173093 | Accuracy: 0.5070650387063993\n",
      "Cost: cross | Activation: sigmoid | Layers: 2 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: cross | Activation: sigmoid | Layers: 3 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: cross | Activation: sigmoid | Layers: 4 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: cross | Activation: sigmoid | Layers: 5 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: cross | Activation: sigmoid | Layers: 6 | F1 Score: 0.22433801546936477 | Accuracy: 0.4954755357623218\n",
      "Cost: cross | Activation: sigmoid | Layers: 7 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: cross | Activation: linear | Layers: 2 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: cross | Activation: linear | Layers: 3 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: cross | Activation: linear | Layers: 4 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: cross | Activation: linear | Layers: 5 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: cross | Activation: linear | Layers: 6 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n",
      "Cost: cross | Activation: linear | Layers: 7 | F1 Score: 0.45529321393375305 | Accuracy: 0.5045244642376782\n"
     ]
    }
   ],
   "source": [
    "cost_functions = ['quadratic', 'cross']\n",
    "activation_functions = ['sigmoid','linear']\n",
    "num_hidden_layers= [x for x in range(2,8)]\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "results = []\n",
    "for cost in cost_functions:\n",
    "    for activation in activation_functions:\n",
    "        for num in num_hidden_layers:\n",
    "            params = dict(n_hidden=30, \n",
    "              n_hidden_layers=num,\n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=500, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1,\n",
    "              cost_function=cost,\n",
    "              activation_method=activation)\n",
    "            nn = MultiLayerPerceptron(**params)\n",
    "            nn.fit(X_train, y_train)\n",
    "            yhat = nn.predict(X_test)\n",
    "            f1score = f1_score(y_test,yhat,average='weighted', sample_weight=weights)\n",
    "            acc = accuracy_score(y_test,yhat)\n",
    "            print(\"Cost:\", cost, \"| Activation:\", activation, \"| Layers:\", num, \"| F1 Score:\", f1score, \"| Accuracy:\", acc )\n",
    "            results.append({'cost':cost, 'activation':activation, 'layers':num, 'f1_score':f1score, 'accuracy':acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2168b37b748>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2168ae21ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df['f1_score']= df['f1_score'].astype(np.float64)\n",
    "df['cost_activation']=df['cost'] + \" \"+ df['activation'] \n",
    "sns.swarmplot(data=df, y='f1_score',x='layers', hue='cost_activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above plot, most of the parameterizations lead to around a 45.5% F-Score. There are a few with around 22% but there is a distint parameter with nearly 69% on there F-score! This parameter used a Quadratic function with a linear activation and 3 layers. We can observe from the plot above that the addition of hidden layers does not increase the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Magnitude Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_hidden=30, \n",
    "              n_hidden_layers=3,\n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=10, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1,\n",
    "              cost_function='quadratic',\n",
    "              activation_method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 10/10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.6631609229757599\n",
      "F1 acc:  0.6865919602019581\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "nn = MultiLayerPerceptron(**params)\n",
    "nn.fit(X_train, y_train, print_progress=10)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Test acc:',accuracy_score(y_test,yhat))\n",
    "sample_weights = [1.5,1.0,2.0,2.5]\n",
    "weights = [sample_weights[x] for x in y_test ]\n",
    "print('F1 acc: ', f1_score(y_test,yhat,average='weighted', sample_weight=weights) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x216f9e4b4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "for i in range(0,params['n_hidden_layers']):\n",
    "    plt.plot(np.abs(nn.gradients_[i]), label='w' + str(i) )\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the average gradient magnitude versus the number of training iterations for my most optimial parameters: 3-layers, quadratic cost function, and linear activation. We can easily see that the magnitude of the gradient start at much different values and rapidly approaches 0. After only a single iteration, the gradients in all 3 layers converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
